%%-----------------Chapter 4--------------------------- 
\chapter{Implementation}

This chapter deals with the implementation details of the system and the
algorithms that make it up. This includes specifics on the structure of the
input data, the training setup and procedures, the learning and adaptation
specifics, and how the testing was performed. Whereas the last chapter brought
up the design decisions and the reasons behind them, this chapter deals with how
the ideas were implemented in the system. This chapter also discusses the outcomes of
the various implementation choices and attempts to further justify the design
decisions.

Before jumping into the specifics of the test implementation, a quick reminder
of the problem domain is in order.  Given a sentence and a target ambiguous word in
the sentence, can a system properly disambiguate the word, providing the correct
word sense?  Therefore, we are trying to disambiguate a word with multiple word
senses in a given sentence, where disambiguation is the action of determining
which word sense is correct in the given situation.

\section{Input Data} 

The input data to the test were taken from the Natural
Language Tool Kit (NLTK), an open source python project for use in linguistics
and natural language processing research and development.  The NLTK is used
specifically for its symbolic and statistical natural language processing, which
is a good fit for the topic of this thesis.  The main use of the NLTK was to
provide the Senseval-2 contest dataset. The dataset is made up of manually
tagged sentences made publicly available through the Senseval/Semeval workshops,
a set of workshops put on by the Association for Computational Linguistics to
promote the semantic analysis of text. These sentences are tagged in a
reproducible manner. Each sentence was added with the rule that human taggers
must agree on the sense of each word at least 90\% of the time, requiring at
least two taggers to determine the validity of each word.  The Senseval-2
dataset consists of 15225 sentences, containing an average of 27 components.  The
sentences were organized in a strict XML format, with each record containing the
following elements:

\begin{itemize}     
	\item word- the targeted word and its abbreviated part of speech. In the 
	Senseval-2 data set there are 4 words to be disambiguated: ``hard'', 
	``interest'', ``line'' and ``serve''. An example of this category is simply
	``Hard-a'', with the ``-a'' being representative of its part of speech (in 
	this case, an adverb)
	\item context- the sentence in which the targeted word is found. This consists of 
	a set of tuples comprising each component of the sentence; each tuple 
	contains a stemmed word or component, and its part-of-speech tag. For 
	instance: 
	\begin{verbatim}
		[('someone', 'NN'), ('has', 'VBZ'), ('to', 'TO'), 
		('stop', 'VB'), ('him', 'PRP'), ('.', '.')] 
	\end{verbatim}

	\item senses- The word senses of the targeted word, taken from the WordNet 
	1.7 sense inventory. An example is ``HARD1'', which can be cross-referenced 
	for its definition. 
\end{itemize}

In addition to the dataset, the NLTK also provided a mechanism for stemming and
lemmatizing each word, as well as providing a list of commonly used stop words.
This allows each sentence to be more compact and concise, which greatly reduced
the overall complexity of the graph-based memory component.

\section{Setup}

The data were set up using five-fold cross validation, in which four subsets of
the dataset were used for training and the fifth was used for testing. This
was then repeated five times, allowing each of the data sets to be the test
data, and having every combination used for the training data.  This method
differed from the method used at the Senseval-2 competition, but allows for an
acceptable comparison.  The Senseval-2 competition gave the researchers a full
training set, but held back on releasing to them the data they would be testing.

\section{Training}

The training component of the system provided a non-trivial challenge.  To
create and synthesize the learning process in humans, a two-phase approach to
training was required.  The first phase consisted of the initial setup of both
the statistical component and the connective component. The connective component
required its ``memory connections'' to be added.  This was the process of adding
weighted edges between words that were found in the same sentence, where the
edge weight was directly proportional to the frequency of occurrences between
the two words.  The statistical component collected predefined information that
was to be used later for decision making.  This first phase ran over the entire
set of training data.

Once this initial processing was done, a second pass of the training material
was required.  This pass emphasized the learning process.  In this phase, the
graph component set up in the previous phase was further adapted and trained to
the dataset.  The statistical component was tested for accuracy and the meta-learning component began its initial setup and data collection on information
from the accuracy of the two other components. This included having the meta-learning component query the statistical component to learn which words in the
sentences were the best on which to disambiguate. This process attempted to eliminate
noisy or unimportant words and select only those words with the best chance to
properly disambiguate the target word.  Selecting the best words was done by
using an algorithm that made use of the following three factors:

\begin{itemize}
	\item Completion rate: this was determined by how likely the
word was to complete successfully. This could also be considered the
relationship between the word and the successful selection of some word sense in
both the statistical and connective algorithms.      
	\item Accuracy: the probability the word would be accurately disambiguated.     
	\item Rarity: the frequency with which a word occurs
\end{itemize}

Finally, once the best words were known, the number of words to choose per
sentence was then determined by the meta-learning system.

Another important aspect of the training phase is the reporting method.  This is
done through the use of a Learning Environment Data Structure (LEDS).  This data
structure carries the information used to solve the problem, collects the
statistics and computes the meta-data. This data structure is passed into both
the statistical and connective guessing process; it collects their choices and
other data required to be updated after the learning instance is completed.  It
is then passed into the meta-learning phase; in this phase, it is used to help
compute the final answer. Finally, after the algorithm has made its guess, it
captures the correct disambiguation, and is used to update the long-term memory
data structure with the changes noted in the learning instance phase.

\subsection{Graph-based adaptation}

The method in which graph-based adaptation took place was through the use of
collected meta-data and instance-based feedback.  With the initial graph
setup, all words in each sentence were interconnected. Over the training data,
this formed a very dense graph, with the value for each edge allowing for the
most likely location for a given word sense to be found.  The idea behind the
algorithm was that each word selected as important in a sentence would vote on which
disambiguation was most likely; the word sense with the greatest number of votes
would be determined as correct.  This technique not only determined the ``best''
possibility, but gave an approximate confidence value in the chosen word sense. The
search performed went through each of the selected starting words, and would
look through its neighbours for the given word senses in decreasing edge weight
order. If an occurrence was found, it was selected as its vote; otherwise it
would pick the highest rated edge and perform the same search on its neighbours.
If, after a certain number of neighbours were checked and no
disambiguations were found, the selected word would not vote.  If in returning
from the search there was a tie, the vote would be decided arbitrarily.
Throughout this search, the path each word took and the disambiguation it voted
for was tracked and kept, to be used to adapt the graph data structure.

The graph adaptation algorithm was performed after the instance problem was
completed and the answer was known.  During this phase a few outcomes were
possible: 

\begin{itemize}      
	\item The graph component correctly determined the word and each word 
	completed its search.  In this case, each edge along the path each selected 
	word took had its weight increased by a pre-determined amount.
	\item The graph component incorrectly determined the word and each word 
	completed its search. In this case, each edge along the path that led to the 
	wrong outcome would have its edge weight value decreased by a 
	pre-determined  amount. This amount was often significantly larger than the 
	increase value, in an attempt to offset initial training imbalances. The 
	remaining words, if any, would not have their values adjusted.     
	\item The graph component correctly guessed the word, but the majority did 
	not finish their search. In the case where confidence is low, the graph 
	component would not submit a vote, but would keep its best guessed value to 
	help determine feedback. In this case, each edge that completed with the 
	correct value had the value of the edge weights along its path increased 
	(usually by quite a bit more than a normal correct case). The edges that did 
	not finish would have the edge weights along their edge decreased by a 
	more significant amount.     
	\item The graph component guessed incorrectly and the majority did not 
	finish their search. In this case each of the values of the edge weights 
	along all of the paths would be decreased, which 
	in most cases was usually the largest reduction. 
\end{itemize}

After completing the second run of the training dataset, the words were known and
the edge weights of the graph had been trained enough for use.  Although it has
not been confirmed if additional runs of the graph adaptation phase would be
beneficial, the length of time this phase takes and the increased likelihood
of overtraining were the deciding factors in using one run.

\subsection{Statistical adaptation}

In keeping with the unconscious mind, the statistical method was designed
specifically to be a quick, relatively simple calculation with decent results.
The best method for this, and one that seemed directly related to the actual
process, was Bayesian statistics.  After each learning instance, the statistical
information contained therein was added to the previously collected
information.  This information was immediately available for use in calculating
probabilities for the next learning instance.  The first part of the learning
phase, the statistical component, collected the number of times words were seen
together, attributing each sentence and each word in the sentence to its proper
word sense.  The second
phase, the optimization component, attempted to select
the correct disambiguation based on prior knowledge, and would again add the
information to its statistical collection.  In the initial system design, the selection algorithm 
used a wider range of variables to determine which word sense was most likely;
this, however, proved to be a poor choice, as the standard selection algorithm
(most frequently selected) worked much better.  This led to a revised version of the system, using the very simple
``most frequently selected'' algorithm.  This algorithm
uses a frequency table to keep track of all of the different word senses of a
single word; when asked to disambiguate a given word, the algorithm simply returned the
word sense that has the highest number of occurrences.  As mentioned above, the
statistical component is also used to select which words are the best words to
use in a sentence to determine the disambiguation. This information is collected
in both phases, but only used in the second phase.

\subsection{Meta-data collection and adaptation}

During the optimization component of the learning phase, the meta-data
collection and adaptation are completed.  This phase consists of two important
parts: 

\begin{itemize}
\item answer biasing- This phase adds a bias to using the answer provided by a 
component in accordance with how accurate the component is.  This uses 
information collected through the meta-data analysis of all previously answered 
learning instances to give weights to each of the answering components 
equivalent to their accuracy.     
\item adaptive processing- This component monitors and tests the pre-defined 
meta-values.  At each phase, a meta-value is randomly selected from the list of 
all meta-values; it is incremented based on a pre-defined ``step'' value and 
then is run for a number of instances. If, after the test run, the changed value 
being tested performs better than average, it uses this new value as a baseline. 
If it performs exceptionally, it attempts to retest the value with an even larger 
value.  If the changed value performs worse, it reverts the value. If it performs 
exceptionally worse, it attempts to retest the value with a negative ``step'', a 
smaller value. 
\end{itemize}

The meta-variables on which these tests were performed consist of two groups. The
first group is the ``desirably controlled quality'' group.  This group
consists of values that are not directly related to accuracy, but that likely have an effect
on the results.  The main reason these were added was to have the system
optimize itself throughout the learning phase and into the more efficient phase.

Desirably controlled quality:

\begin{itemize}
\item important words - the number of words selected from a sentence used to 
determine the word sense. This one was selected in hopes that, as the algorithm 
became more efficient, or during the initial learning phase, the number of words 
required could shrink or grow. This would allow the algorithm to determine when 
it should use more words and run more slowly, while maintaining or increasing 
accuracy, or when it should use less words and run faster without affecting 
accuracy.     

\item iterations between considerations - This represents the number of test 
instances between the meta-data phases. It was expected that this value would be
 the last value selected once the other values found an equilibrium, as any 
 results other than this would lower accuracy. Once the equilibrium was found, 
 this value would increase, optimizing the speed of the test.     
\item graph depth - This is the maximum depth to which the graph algorithm would go before 
guessing.  As with important words, it was thought that it would find the best 
compromise for accuracy initially, and as the graph became more accurate, it 
would be increased to allow for faster
runtimes.

\end{itemize}

The second group is the ``inflation limiting'' group.  This group consists of
values that can directly affect accuracy, and that were added to the meta-variables
specifically to counter inflation of edge weight values over time.  The hopes
were that this would combat dataset biases.

Inflation limiting: 
\begin{itemize}     
\item Positive Outcome - This was the value gained for each edge used in 
determining a correct word sense disambiguation.      
\item Negative Outcome - This was the value removed for each edge used in 
determining an incorrect word sense disambiguation.
\end{itemize}

These groups and the meta-learning component allowed for a great chance to add
accuracy in modelling the biological/cognitive aspect of memory and learning,
with a great chance of increasing overall accuracy and efficiency. Along with
this, there was also a number of possible benefits such as a scaling reward and
punishment system, and an algorithm that was slower and more deliberate in its
searching when its accuracy was suspect, and faster when the learning component
was in the ``zone''.  It also had the added effect of eliminating the guess work
in initial assignment of these values.
