%%-----------------Chapter 4--------------------------- 
\chapter{Implementation}

This chapter deals with the implementation details of the system and the
algorithms that make it up. This includes specifics on the structure of the
input data, the training setup and procedures, the learning and adaptation
specifics, and how the testing was performed. Whereas the last chapter brought
up the design decisions and the reasons behind them, this chapter deals with how
the ideas were implemented in the system. This chapter also discusses the outcomes of
the various implementation choices and attempts to further justify the design
decisions.

Before jumping into the specifics of the test implementation, a quick reminder
of the problem domain is in order.  Given a sentence and a target ambiguous word in
the sentence, can a system properly disambiguate the word, providing the correct
word sense?  Therefore, we are trying to disambiguate a word with multiple word
senses in a given sentence, where disambiguation is the action of determining
which word sense is correct in the given situation.

\section{Input Data} 

The input data to the test were taken from the Natural
Language Tool Kit (NLTK), an open source python project for use in linguistics
and natural language processing research and development.  The NLTK is used
specifically for its symbolic and statistical natural language processing, which
is a good fit for the topic of this thesis.  The main use of the NLTK was to
provide the Senseval-2 contest data set. The data set is made up of manually
tagged sentences made publicly available through the Senseval/Semeval workshops,
a set of workshops put on by the Association for Computational Linguistics to
promote the semantic analysis of text. These sentences are tagged in a
reproducible manner. Each sentence was added with the rule that human taggers
must agree on the sense of each word at least 90\% of the time, requiring at
least two taggers to determine the validity of each word.  The Senseval-2
data set consists of 15225 sentences, containing an average of 27 components.  The
sentences were organized in a strict XML format, with each record containing the
following elements:

\begin{itemize}     
	\item word- the targeted word and its abbreviated part of speech. In the 
	Senseval-2 data set there are 4 words to be disambiguated: ``hard'', 
	``interest'', ``line'' and ``serve''. An example of this category is simply
	``Hard-a'', with the ``-a'' being representative of its part of speech (in 
	this case, an adverb)
	\item context- the sentence in which the targeted word is found. This consists of 
	a set of tuples comprising each component of the sentence; each tuple 
	contains a stemmed word or component, and its part-of-speech tag. For 
	instance: 
	\begin{verbatim}
		[('someone', 'NN'), ('has', 'VBZ'), ('to', 'TO'), 
		('stop', 'VB'), ('him', 'PRP'), ('.', '.')] 
	\end{verbatim}

	\item senses- The word senses of the targeted word, taken from the WordNet 
	1.7 sense inventory. An example is ``HARD1'', which can be cross-referenced 
	for its definition. 
\end{itemize}

In addition to the data set, the NLTK also provided a mechanism for stemming and
lemmatizing each word, as well as providing a list of commonly used stop words.
This allows each sentence to be more compact and concise, which greatly reduced
the overall complexity of the graph-based memory component.

\section{Setup}

The data were set up using five-fold cross validation, in which four subsets of
the data set were used for training and the fifth was used for testing. This
was then repeated five times, allowing each of the data sets to be the test
data, and having every combination of sections used for the training data.  This method
differed from the method used at the Senseval-2 competition, but allows for an
acceptable comparison.  The Senseval-2 competition gave the researchers a full
training set, but the testing data were withheld until after the competition was completed.   

\section{Training}

The training component of the system provided a non-trivial challenge.  To
create and synthesize the learning process in humans, a two-phase approach to
training was required.  The first phase consisted of the initial setup of both
the statistical component and the connective component. The connective component
required its ``memory connections'' to be added.  This was the process of adding
weighted edges between words that were found in the same sentence, where the
edge weight was directly proportional to the frequency of occurrences between
the two words. Essentially this built the unoptimized long-term memory graph, with a
simple estimate for the edge weight, the number of times the two words appeared together 
in a sentence.  The statistical component collected word-based frequency information 
that would be immediately relevant for problem solving.  This first phase ran over 
the entire set of training data.

Once this initial processing was done, a second pass of the training material
was required.  This pass could be considered the learning phase and has the goal 
of optimizing the edge weights of the memory graph, verifying the statistical 
component's accuracy, and balancing the weights behind the meta-learning component's 
decision making process.  To optimize the memory graph, the training data were used 
again. With the structure of the graph complete, the disambiguation was attempted 
by using various paths through the graph.  If the paths were successful at disambiguating 
the word, the edge weights were increased; otherwise they were decreased. The 
statistical component was also checking its accuracy throughout the second pass of 
the training data.  At the end of the second pass, both 
components had an accuracy associated with them. These accuracies were then 
passed into the meta-learning component, which used them to help determine how to 
select the best outcome, given the results of the statistical and graph-based components.  
Furthermore, using the statistical component's word frequency information, the 
meta-learning component could determine which words in a given sentence would most 
likely end in a successful word-sense disambiguation. 

In order to select the best words from the sentence, the following three factors, based on both the 
word in question and the word to be disambiguated (target word), had to be considered: 

\begin{itemize}
	\item Accuracy: the probability the target word would be accurately disambiguated.
	\item Completion rate: the number of times the word has been involved in a 
	successful disambiguation.  
	\item Rarity: the frequency with which the word occurs.
\end{itemize}

In the end, the algorithm used is:

\IncMargin{1em}
\begin{algorithm}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{accuracy}{accuracy}
\DontPrintSemicolon
\Input{The target word $T$, a word to compare to $W$}
\Output{A value of how ``good'' $W$ is in relation to $T$}
\BlankLine
	$N \longleftarrow 0$\;
	$D \longleftarrow 0$\;
	\ForEach{$t$, a word sense of $T$,}{ 
		$N \longleftarrow N$ + number of times $t$ has been in a sentence with $W$\;
		$D \longleftarrow D +$ total number of words $t$ has seen\;}
	\Return $(N / D) *$ \accuracy{$W$}\;
\caption{best word algorithm \label{ALG_BESTWORDS}}
\end{algorithm}\DecMargin{1em}

Where the accuracy function is defined as the number of successful disambiguations $W$ has 
been involved in.

Finally, once the best words were known, the number of words to choose per
sentence was decided by the trial and error method in the meta-learning component.

\subsection{Graph-based adaptation and query}

The method by which graph-based adaptation and queries took place was through the use of
collected meta-data and instance-based feedback.  In the initial graph
setup, the words in each sentence were used as nodes. The number of times two words 
were seen together was the initial weight of the edge between them. During the second pass 
of the training data and during the test phase, the graph was used to discover the best 
word sense. It did this by beginning a search on each node related to the chosen 
best words of a sentence. The algorithm first looked to see if a disambiguation 
of the target word existed in its neighbours in an edge-weight-descending order. 
If one did not, it traversed the most highly-weighted edge and looked at the new node 
in a similar manner.  When the search ended, each node had a candidate 
word sense, and the word sense with the greatest tally was returned.

More formally the algorithm is:

\IncMargin{1em}
\begin{algorithm}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwFunction{Dis}{disambiguationOf}
\SetKwFunction{Sort}{sortedge}
\SetKwFunction{Neighbour}{neighbour}
\SetKwFunction{Break}{break}
\SetKwFunction{GetLargestCardinalElement}{getlargestcardinalelement}
\DontPrintSemicolon
\Input{The target word $T$, a list of word nodes related to a sentence $L$, a max depth $X$}
\Output{A word sense of $T$}
\BlankLine
	List $Y$\;
	\ForEach{$n$ in $L$}{
		\If{$n$.count $ + 1 > X$}{
			$Y \longleftarrow \emptyset$\;
			\Break\;
			}
		\Sort{$n$, Descending}\;
		\ForEach{neighbour $s$ of $n$}{
			\If{\Dis($T$, $s$)}{
				$Y \longleftarrow s$\;
				\Break\;
				}
			}
		$u \longleftarrow $ \Neighbour{$n$}\;
		$u$.count $\longleftarrow u$.count $+ 1$\;
		$L \longleftarrow u$\;
		}

	\Return \GetLargestCardinalElement{$L$}\;
\caption{graph search\label{ALG_GRAPHSEARCH}}
\end{algorithm}\DecMargin{1em}


Algorithm \ref{ALG_GRAPHSEARCH} presents how the graph component is used to 
disambiguate a word; the adaptation component requires a bit more background. 
The adaptation phase is the process of modifying the edge weights of the graph 
to better the outcome of subsequent graph queries or searches. The graph adaptation 
algorithm is performed after the problem instance has completed and the correct 
disambiguation is known.  During this phase, the outcomes are: 

%Simplified this, adding the guessing cases seems like overkill, when all it essentially does is penalize
%subjective components a bit more or less.
\begin{itemize}      
	\item The graph component correctly determines the word sense. 
	In this case, the edge weights along the path each selected 
	word took has its value increased.
	\item The graph component incorrectly determines the word sense. 
	In this case, each edge weight along the path that led to the 
	wrong outcome would have its value decreased. This value was often significantly larger than the 
	increased value for successful outcomes.      
\end{itemize}

In keeping with the human memory modeling, the values used to increase and decrease the edge 
weights were set using the trial and error method in the meta-learning component.

After completing the second run of the training data set, the words were known and
the edge weights of the graph had been trained enough for use.  Although it has
not been confirmed if additional runs of the graph adaptation phase would be
beneficial, the length of time this phase takes and the increased likelihood
of overtraining were the deciding factors in using one run.

\subsection{Statistical adaptation}

In keeping with the unconscious mind, the statistical method was designed
specifically to be a quick, relatively simple calculation with decent results.
The best method for this, and one that seemed directly related to the actual
process, was Bayesian statistics.  After each learning instance, the statistical
information contained therein was added to the previously collected
information.  This information was immediately available for use in calculating
probabilities for the next learning instance.  The first part of the learning
phase, the statistical component, collected the number of times words were seen
together, attributing each sentence and each word in the sentence to its proper
word sense.  The second
phase, the optimization component, attempted to select
the correct disambiguation based on prior knowledge, and would again add the
information to its statistical collection.  In the initial system design, the selection algorithm 
used a wider range of variables to determine which word sense was most likely;
this, however, proved to be a poor choice, as a simple most-frequently-selected heuristic worked much better.  
This led to a revised version of the system, using the very simple
``most frequently selected'' algorithm.  This algorithm
uses a frequency table to keep track of all of the different word senses of a
single word; when asked to disambiguate a given word, the algorithm simply returned the
word sense that has the highest number of occurrences.

\subsection{Meta-data collection and adaptation}

During the second run of the training phase, the meta-data collection 
and adaptation are completed.  This phase consists of two important
parts: 

\begin{itemize}
\item answer biasing- This phase adds a bias to the answers provided by the 
two other components in accordance with past accuracy.  This uses information 
collected in previously answered instances to add weights to each of the 
answers provided by the components. It uses these weights and the confidence 
each component has in its answer to choose the final word sense.      
\item adaptive processing- This component monitors, tests and adapts the 
pre-defined meta-values.  After each problem instance, a check is done to 
see if processing is required.  If processing is required, a meta-value is 
chosen to go through a trial and error testing phase.  This phase involves 
modifying the value by a pre-defined ``step'' value.  Training or testing 
continues with this value set; after a number of instances, the processing 
phase is triggered again. The results from this testing phase are compared 
against the captured performance statistics. If the test returns a significant 
improvement (greater than a standard deviation), the new value is set as 
default and the meta-value is again selected for testing. If the test 
returns a failure, the value is reset and another is randomly chosen. Finally, 
if the value sees a moderate improvement, the value is set and a random 
meta-value selected for testing.
\end{itemize}

The meta-variables on which these tests were performed consist of two groups. The
first group is the ``desirably controlled quality'' group.  This group
consists of values that are not directly related to accuracy, but that likely have an effect
on the results.  The main reason these were added was to have the system
optimize itself throughout the learning phase.

Desirably controlled quality:

\begin{itemize}
\item important words - the number of words selected from a sentence used to 
determine the word sense. This one was selected in hopes that, as the algorithm 
became more efficient, or during the initial learning phase, the number of words 
required could shrink or grow. This would allow the algorithm to determine when 
it should use more words and run more slowly, while maintaining or increasing 
accuracy, or when it should use fewer words and run faster without affecting 
accuracy.     

\item iterations between considerations - This represents the number of test 
instances between the meta-data processing phases. It was expected that this value would be 
the last value selected once the other values found an equilibrium, as any 
results other than this would lower accuracy. Once the equilibrium was found, 
this value would increase, optimizing the speed of testing.

\item graph depth - This is the maximum depth to which the graph algorithm would go before 
guessing.  As with important words, it was thought that it would find the best 
compromise for accuracy initially, and as the graph became more accurate, it 
would be increased to allow for faster runtimes.

\end{itemize}

The second group is the ``learning optimization'' group.  This group consists of
values that are too hard to set statically, and would likely have a moving optimal 
target. The main reason these were added was to conform to human memory models, as well 
as the inability to determine an optimal value.

Inflation limiting: 
\begin{itemize}     
\item Positive Outcome - This was the value gained for each edge used in 
determining a correct word sense disambiguation.      
\item Negative Outcome - This was the value removed for each edge used in 
determining an incorrect word sense disambiguation.
\end{itemize}

These groups and the meta-learning component allowed for a great chance to add
accuracy in modelling the biological and cognitive aspect of memory and learning,
with a likely increase in overall accuracy and efficiency. Along with
this, there was also a number of possible benefits such as a scaling reward and
punishment system, and an algorithm that was slower and more deliberate in its
searching when its accuracy was suspect, and faster when the learning component
was in the ``zone''.  It also had the added effect of eliminating the guess work
in initial assignment of these values.
