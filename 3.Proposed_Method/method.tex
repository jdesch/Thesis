%%--------------------Chapter------------------------ 
\chapter{Methodology}

This chapter discusses biological and psychological models and constructs that
may have a positive impact on natural language processing and the word sense
disambiguation problem.  This chapter outlines the current system, the design
decisions required by the models, and the architecting required to make the
systems work together.  This chapter attempts to show the number of design
decisions that have to be made to balance the two accuracy goals of this thesis,
the accuracy of the results and the accuracy of the model. Further details
include what changes were required to get the models to work together, how the
current system incorporated all the components together, and what is lacking in
the current system.  The implementation details of the various algorithms,
structures and components are left to Chapter 4.

In the general case, psychological and biological models related to the human
brain and its learning capability lay out various ``executive'' functions.
These functions are high-level objectives that, when intelligently combined, allow
the mind to recall past events, solve problems, and learn. The executive
functions themselves are made up of many other ``building block'' functions that
support the goal of the executive functions and allow generic and adaptive
problem solving.

Much about the mind's systems is a mystery to researchers, with insights coming
from logically deductive models.  Models are reviewed by the scientific
community, and those that are perceived to work with what is known are given
more confidence.  With little describing the building block functions, coming up
with a solution to all of the minor problems in this thesis's system was based on
simplicity, good software practices and artificial intelligence research.

Through the course of the thesis work, there was a continual readjustment of the
tradeoffs. The competing design points were: complexity, implementation time,
faithfulness to the models and, when results became available, accuracy in the
results.  The initial designs were too complex and required too much time for
little gain in either of the accuracies.  After a few iterations of refining the
core idea and removing some of the parallelism complexity, a plausible model was
designed and implemented.

\section{High-level solution}

\begin{figure}[htp]     
	\begin{center}
		\includegraphics[width=0.98\linewidth]{figures/models/System}
		\caption{Proposed Design 
		\label{fig:high_level_solution}}
	\end{center}
\end{figure}

Figure \ref{fig:high_level_solution} gives a high level overview of the learning
process.  The system begins by receiving a sentence and a target word to disambiguate. 
It then preprocesses the sentence by removing stopwords, then normalizes and
stems the remaining words.  After this, the sentence is run through a ``best word
choice'' algorithm.  This algorithm attempts to determine which words are the
most ``important'' and how many of the words should be used to determine the
sense. After the words are chosen, they are then added to short term memory and
loaded from working memory.  From the working memory graph, the nodes are passed
to a search function that simulates a cognitive thought process (linking
memories from one to another) until a result is found.  At the same time, a
``gut check'' or fast statistical method is used on the selected words to
determine what is the likely disambiguation. These methods return their best
guess and a confidence score.  These are passed into the meta-learning algorithm, 
which uses the performance of both algorithms on the current dataset to modify
the final disambiguation/confidence value.  After the answer is passed on, the
statistics collected for the current problem instance, through the use of the
short term memory object, are folded into the working memory and statistical
engine.  Finally, a check is done to determine if a meta-AI update is required.
If it is required, the system evaluates the accuracy since the last update, and
determines whether it should increase the current selected value, return it to a
previous state, or randomly select a different meta value.

\section{Current Model}

The high level solution mentioned above gives an executive view of the system.
This section goes into detail of what the components are, what design decisions
were made, and how the system differs from the ideal solution.

\begin{figure}[htp]     
	\begin{center}
		\includegraphics[width=0.97\linewidth]{figures/models/Component}
		\caption{Component Overview}     
	\end{center}     
	\label{fig:Component}
\end{figure}

The current model consists of three functional components, a higher level
container object and a ``short-term'' memory data structure.  The Statistical,
Connective, and Meta-learning components are used by the Memory component to
provide access to the various ``executive'' building blocks.

\subsection{Memory object} 

The memory object could be considered the interface
to learning. It contains methods that provide the following functionality:

\begin{enumerate}     
	\item a way to load and save the state of the mind. This way, the learning
	environment (specifically the meta-learning information, but the other 
	two components as well), can be stored for further refinement against 
	additional datasets. This more closely mimics the environment in which 
	humans learn.
	\item a training method, which given a set of data, will add the
	information to the mind and attempt to optimize the learning of the data.
	\item an answer method, which queries the functional components to find the 
	best answer given the initial information. 
\end{enumerate}

While nothing overly complex is going on, ideally this object would dynamically
and based on prior problem solving information, design the requirements for the
solution. It would then pass the various components enough information to have
them intelligently look for the answer.  Another ideal characteristic would be
for the memory object to handle the parallel processes and synchronize
information as it was found.  This would allow biases based on subconscious
insight, or new information coming into focus to change the solution path,
allowing a more elastic problem solving capability.  These enhancements would
also bring the memory object more inline with Baddeley and Hitch's Central Executive.

\subsection{Statistical component} 

The statistical component is an attempt to
mimic the subconscious mind, a quick pattern matcher that is the first
component of memory to have a possible solution to a problem.  Tests have shown
\cite{BLINK} that the subconscious mind is quick to realize patterns, but slow to
affect conscious decisions. Ideally it would be continually collecting
statistics, running over problems, and attempting to make sense of how
disconnected information possibly fits together.  While solutions can come in
the form of an often indescribable certainty \cite{BLINK}, depending on your
expertise in the area, they are often correct.

The statistical component is responsible for the following functionality:

\begin{enumerate}     
	\item a method to capture and train the words.  This method counts and 
	indexes the words in their relation to the target disambiguation. It also 
	captures statistics on each of the targeted word-disambiguation 
	combinations.     
	\item a voting mechanism, which given a list of words and a target, uses 
	frequency and accuracy statistics of each word against each of the 
	disambiguations of the target word.
	\item a method to determine which words in a sentence are most likely to 
	lead to a successful completion.  This determines the importance of each 
	word currently based on the percentage of times a word has been seen with 
	the targeted word.     
	\item a method that determines a best guess, or given nothing but the target 
	word, what is the best disambiguation.
\end{enumerate}

While the model is currently simplistic, this is a necessity with the constraints of current computer
hardware as it relates to the massive parallelism of the human brain and general
solution complexity.  Ideally, the statistical component or subconscious mind
would collect and index every piece of statistical information it could find 
and, using all non-crucial resources, continually refine and apply new information
to existing problems.  This would allow the mind to give it enough information
for it to make a ``gut check''.

\subsection{Connective component} 

The connective component models long-term
memory, a logically organized data structure that attempts to allow a deductive
problem solving process.  The connective component in combination with the
working-memory component allows for ``memory'' connections to gain strength or
to decay, increasing the success rate of determining the deductive path.

The connective component is responsible for the following functionality:

\begin{enumerate}
	\item a method to capture/learn the words.  This method simply loads the 
	problem instance's words as a complete network into the memory graph, 
	augmenting existing nodes/edges as required.     
	\item a voting mechanism. In this method, each of the chosen words gets to 
	vote on which disambiguation it believes is correct.  The voting currently 
	occurs through the first target word found while traveling the memory structure 
	using a modified Djikstra's algorithm. 
\end{enumerate}

In addition to the above functionality, the data structure itself allows for
feedback from the working memory object.  This feedback currently allows nodes
that were successfully/unsuccessfully used to find the solution to
increase/decrease the edge weight between the selected node and target node.
This is designed to effectively model the self imposed reward/penalty
(sureness/doubt) which comes with properly or improperly using information in
forming a solution.

The expressiveness of a graph in modelling long term memory does allow for a
natural correlation. While the form that long-term memory takes, how it is used in
problem solving, and how it works with working memory is fairly ambiguous in any
of the models, a number of improvements could be made to bring the memory more
inline with the models:

\begin{enumerate}     
	\item dimensionality - Currently all items are mapped to one dimension, and the 
	node's edge weights are valid for the entirety of the problem set.  It is my
	understanding that the edge weights should be biased on meta information 
	used in defining the problem.  This information would segment certain 
	components of the edge weight, allowing the graph to be modified based on 
	the context of the problem.  Currently this was set aside as the graph is 
	already slow enough, and the meta-learning component is not able to 
	learn/tag individual problems as related to other problems.    
	\item subconscious access - As mentioned above, in a parallel environment 
	similar to the brain's, ideally the subconscious working away would affect 
	the memory graph (slowly), applying information learned in pattern matching 
	to the overall structure of the graph.
\end{enumerate}

\subsection{Meta-learning component} 

The meta-learning component is an attempt
to bring some of the functionality of the central executive and methodology of
problem solving to the current model. It is responsible for various meta-values
that are added into the algorithms of the other components. Currently there are 
only a few; however, this reasonably outlines a simplistic model of how the
building block functions or even executive functions could be initially
modelled.

Currently tracked values include:

\begin{enumerate}     
	\item important words - the number of words to keep when evaluating a given 
	sentence for importance     
	\item graph depth - the maximum depth to which the voting algorithm should go when 
	attempting to vote using a word 
	\item graph correctness - the value by which to increment the edge weight 
	between the given node and target node when it is correct     
	\item graph incorrectness - the value by which to decrement the edge weight 
	between the given node and target node when it is incorrect     
	\item update frequency - how many solutions should pass between meta 
	algorithm updates 
\end{enumerate}

Each value contains the current numerical value, the complete history of changes
and the maximum step amount between changes.  The algorithm for meta-learning
simply randomly selects a value to change, changes it, and then uses the hypothesis
testing to determine if the outcome was worth it. If it was, it changes it again; 
if it wasn't, it reverses the value and randomly selects another value to
modify.

In addition to this, the meta-learning component is also used to track how each
of the other two components performs and adjusts their voting confidences
based on past performance, ultimately giving the meta-learning component the final
say on voting success.

It also puts the working memory information learned in each problem instance
into practice, modifying the statistical and graph information contained in the
other two components.

Ideally, expanding on the meta-tracking information to include all values, and
eventually all methods and general problem solving strategies, would allow the
meta learning object to become more like the brain and move from specific
problems to general problems.  The complexity of this increases quickly, as does
the number of resources required (massive parallelism again). The necessary
information to track would grow and, more importantly, the number of training
examples required to generate good results would be very large.
Initially, genetic algorithms would be used just to get off the ground, but tackling the
executive function problems and then the specific component algorithm creation
requires something outside of the scope of this thesis.

\subsection{Learning Environment Data Structure}  

This object simply tracks
required information to solve a problem.  It is in a sense a simplistic short-term memory object.  It is required to be passed into all of the components as
each component requires tracking information.  It is then used to modify the
long-term memory structures.

\section{Base Models}

Now that the system has been outlined at both a high level and a component
level, an overview of the decisions that were required will help the reader to better
understand the complexity of the model. This section analyses the components
against the models, shows their shortcomings and attempts to justify a number
of the decisions made.

\subsection{Baddeley and Hitch}

In the initial implementation, the main component taken from Baddeley and
Hitch's model was the central executive.  The central executive was responsible
for scheduling tasks, coordinating the underlying systems, and making sense of
the results from the sources.  In the final implementation, with the removal of
parallelism, the tasks became more static.  The functions that made up the
original executive got moved into the memory workflow.  Methods that
coordinated the system's tasks and handled the passing of values and components
between the various memory components remained, but were simplified.  With the
reduced constraints due to the serialization of tasks and workflow, the
scheduling component was removed, as the need for dynamic execution was no
longer required.

The looping components of the model were reduced from the three or four
components outlined in Section 2.1 into one main looping structure.  This looping structure
consisted of the train, optimize and guess loop.  The training part initializes
and loads the long-term memory component and the unconscious mind's statistics
engine.  The optimize part simulates one cycle through the ``learning'' loop,
like the Baddeley and Hitch model; information is processed, connections are
made and removed, and the long-term memory structure becomes more organized.
Ideally, just as in the model, this portion of the loop would iterate several
times, strengthening the structural learning components. This process being slow
in humans, who have the luxury of gathering it over many decades, it is also quite
slow for the non-parallelized program (and, even then, work on handling
simultaneous reads and writes would be required). Finally the guess loop uses the
metrics gathered and the long-term and statistical information created in the
optimize part to find the most likely solution to the query. Another aspect
taken from the model is that of influence between components.  Where the
Baddeley and Hitch model parallelized the input, and handled them in specific
memory objects, this project had a single point of input which was then
transferred into two models of viewing it, with the memory component and
unconscious mind component acting as the central executive.

While the overall structure is similar, it is simplified to fit.  The main
aspect of the Baddeley and Hitch model is how the multiple loops and buffers are
processed and how they interact with the various memory components.  The
phonological loop repeatedly rehearses sounds, such as language, to refresh a
phonological area of working memory.  This can be used with sounds and language
themselves, or through reading words and having the phonological loop repeat the
words, putting the words in working memory, strengthening each of the words and
the connections.  While the phonological loop and memory store
are at work, the mind can also be processing other components of the story at
the same time in the visuospatial sketchpad or the episodic buffer. The
visuospatial sketchpad allows visual information to be stored and manipulated,
and tasks involving planning, navigation and physical movement to be ordered and
manipulated.  The episodic buffer is thought to link all the forms of learning
(visual, spatial, and verbal) to a chronological timeline.  This component is also
the most important in linking to long-term memory and making semantic
connections.  Now if we take the reading example mentioned above and apply both
the visuospatial sketchpad and the episodic buffer, we gain a visual memory of
the scene in the book, which we are able to mentally move through.  We also have
an imprint of the sounds of the words and a timeline of how the scene unfolds
and a semantic understanding of the words.  The three systems working together
have a powerful effect of truly understanding the words, the characters and the
scene. With the three acting in concert together, information gained in any of
the sections is available to the mind as a whole, thus compounding the learning
ability and increasing the likelihood of making long-term and unconscious
connections.  With Baddeley and Hitch specifically stating that these components
work in parallel with one another, with any information passing or issues
handled by the central executive, the learning and understanding potential is
much more powerful than the serialized one-track solution designed.

\subsection{Cowan}

The main idea used from Cowan's memory model is the unification of long-term and
working memory through the use of focus.  The memory components have three
states; however, in each state, the memory component has the same internal
representation. The initial state, which occurs through the creation of an
instance, is done when the problem encountered requires a new memory container.
This occurs when the word encountered has not been seen before and could be
required to solve the current problem. The memory component is then used for the
problem instance, and once the instance is completed and the component is found
to be useful, the components state is added to the long-term store. The next
state occurs while the memory component is at rest in the long-term store. When
the memory component is not required for a problem, the component lies dormant
in a long-term store. For the majority of the time, the component is in this
state.  In this state, it can be written to a file, serialized, and
compressed, mimicking the storage condition of the human brain. Finally, in the
third state, the stored memory components are required for a problem instance.
The components that gain focus are activated out of the long-term store and are
brought into the current problem instance with the information of past
experiences available. In the active state, they provide connection to other
nodes that could contain required information, and hold useful statistics used
in the problem solving algorithm.  During the problem instance, interactions
useful to solving the problem are imprinted on the active memory nodes; 
when the problem is completed, the active nodes are returned to the long-term
store, with new problem solving statistics.

One of the limitations of the implementation, in regards to Cowan's model, is the
statistical collection and memory types.  Cowan's model allows for varying
degrees of detail as required by the degree of focus.  In the long-term store,
each component holds little information, but the connection of all of the
components can be seen.  With the first degree of focus, only approximately
seven elements are ``loaded'' into memory, but with this, the amount of
information and statistics on each of the seven elements are much greater.
While the implementation attempted to do this, the information chosen for
collection was hand-selected and focused on what was ``thought'' to be more
useful, and was stored at every level (but not necessarily used).  In order for
this to be more useful, an automated method for statistical collection would have
to be used and analyzed by the unconscious mind.  It is only then that focusing
on a few elements or a single element and allowing a comparison between the
deeper statistical information could garner a much greater level of problem
solving. Furthermore, in only capturing written text input, the deepest focus,
which loads only a single element but allows for a full view of the element, is
missing out on better connections.  Whereas humans can add the input from the
five senses to a memory component, and in doing so create a five dimensional
graph of connections, the implementation deals only in a single dimension, which
suffers from a convoluted word space and unsolvable disambiguations.

\subsection{Ericsson and Kintsch}

The important idea in Ericsson and Kintsch's work is the idea of memory linking.
The idea is that, in working memory, we are able to hold cues or links to long-term memory components.  This allows us to understand complex memories through
the use of holding important short-term concepts which can quickly bring the
more abundant long-term memories into focus.  This model works very well in
practice.  In the working model, each memory component that is loaded into short
term memory is little more than a cue to its place in the long-term memory data
structure.  The memory component contains some information about itself and a
list of its neighbours.  The algorithms for readying the working-memory
structure for problem solving first attempt to load the memory objects (and
therefore cues) that seem most relevant.  Then, in the process of solving the
problems, each component is analyzed, and if during the process of solving the
problem, any of the cues seem overly relevant, the more strongly related
neighbours are loaded into memory.  This allows the relationship of the memory
components seen during the initial look at the problem to be the starting point.
Then, using the complex collected relationships of each memory component's long-term links, it begins traversing the set of both nodes looking for similarities.
When the perceived strongest relationship links to a disambiguation that matches
the word in question, the connective component's search for an answer is
considered complete.

Another important idea in Ericsson and Kintsch's work is that there are
different retrieval structures dependent on the nature of the subject matter contained
therein.  While the working model created uses a combination of the deliberate
structure and the text comprehension structure, it is not capable of separating
the two methods or of applying the pattern and schema's structure.  Separating the
two structures currently in use would allow the model to put more importance on
links that have a higher likelihood of solving a problem.  Instead, the
deliberate structure is used on every potential item in each sentence, making
each item as important as every other item.  It does use text comprehension to
remove items that are not relevant, focusing instead on the nouns and verbs of a
sentence.  To improve the memory structure further, applying the patterns and
schema to the solutions where relevant would greatly increase connective memory
accuracy.  This would require three distinct parsers with three separate
learning engines.  The sentence parser would act as it currently does and would rate
the importance of each word.  At the same time, the deliberate memory parser
would determine if any of the words or ideas in the sentence link to any flagged
memories, triggering their addition to the solution equation.  This portion
would require a method to determine if a given memory component has some value
that an algorithm would determine to be anomalous enough to warrant a deliberate
memory link. Finally, the third parser would be the pattern and schema parser; 
this would abstract and compare the current sentence and ideas within the
sentence to other abstracted memory component collections.  This parser would
then determine likely outcome memory components or memory areas to narrow the
search.

The current model does perform a number of what the parsers described above do; 
however, a more complex memory component model would be necessary to provide the
abstractions needed to determine general problem-solving patterns.  Furthermore,
developing the algorithms to determine which memory components are more
important and should be used as a future indicator of success where possible
requires more research.

\subsection{Unconscious Mind}

The unconscious mind component was initially not a planned component.  It came
from the necessity of having an algorithm that did not require the connective
memory component to determine which words in a sentence were important. In
researching the subconscious mind, and how humans use their ``gut'' in problem
solving, it was evident that a component that tracked simple statistics was
required. The unconscious mind became the basis for determining the best words
in a sentence, the statistical memory component of the overall solution
algorithm, and assisted in the meta-learning algorithm.  While no definitive
models exist for what exactly the conscious mind does, the Iowa gambling task
\cite{Bechara1, Bechara2} mentioned in Blink \cite{BLINK} adds credibility to an
unconscious component of the brain forming a reaction well before it enters
conscious thought. It is assumed that some form of statistical analysis is
performed and it is of great use in decision making.  The idea of the
subconscious as a statistical solution engine guided the design.  The studies on
the use of unconscious mind emphasized the difference between declarative
knowledge, which allows one to articulate reasons for choices, and procedural
knowledge, which allows repeated complex actions to be understood on an
unconscious level.  Where declarative memory takes longer to recall, the mental
processes that act on it allow more useful relative information to be made
available.  On the other hand, procedural knowledge can, over time, move from a
subconscious understanding to a long-term reflexive understanding.  The person
is able to know something or properly mimic an action but it is done at a level
that requires little, if any, conscious thought.  Procedural knowledge is more
disconnected from neighbouring memories, and even an attempt at describing the
process is often difficult.

The implementation of the unconscious mind in the current model does attempt to
add in procedural knowledge.  However, unlike true procedural knowledge, the
implementation's version has a much more conscious process.  Currently, the model
relies on a predefined collection of statistical information.  This information is
then used in a number of predetermined algorithms; some, like the hypothesis
testing algorithm, allow for flexible and adaptable use of information that most do
not.  The ideal solution would allow for emergent behaviours in both determining
what statistics to collect, and how to use the statistics to best solve the
current and future problems.  Another issue that would allow for greater success
is having the unconscious mind completely separate from the other components,
collecting the information and statistics anonymously. This would allow for both
a better collection of information and more up to date information when needed.
The unconscious mind could also spend a great deal more of its time making,
testing and redefining multiple hypotheses, without affecting the conscious work
loop.

The statistical engine in its current state tracks a number of useful properties
including: 

\begin{itemize}
	\item the number of times a word has been seen
	\item the number of times the word has been in a successfully disambiguated
	sentence
	\item the number of times the word has been used successfully in the 
	statistical and connective algorithms
	\item a list of each word that has been seen by a disambiguation, and the 
	number of times it has been seen. 
	\item a list of each disambiguation that a word has seen, and the number of 
	times it has been seen. 
\end{itemize}
