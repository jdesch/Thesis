%%--------------------Chapter------------------------ 
\chapter{Methodology}

This chapter discusses biological and psychological models and constructs that
may have a positive impact on natural language processing and the word sense
disambiguation problem.  This chapter outlines the current system, the design
decisions required by the models, and the architecting required to make the
systems work together.  An attempt is made to show the number of design
decisions that have to be made to balance the two accuracy goals of this thesis,
the accuracy of the results and the accuracy of the model. Further details
include what changes were required to get the models to work together, how the
current system incorporated all the components together, and what is lacking in
the current system.  The implementation details of the various algorithms,
structures and components are left to Chapter 4.

In the general case, psychological and biological models related to the human
brain and its learning capability lay out various ``executive'' functions.
These functions are high-level objectives that, when intelligently combined, allow
the mind to recall past events, solve problems, and learn. The executive
functions themselves are made up of many other ``building block'' functions that
support the goal of the executive functions and allow generic and adaptive
problem solving.

Much about the mind's systems is a mystery to researchers, with insights coming
from logically deductive models.  Models are reviewed by the scientific
community, and those that are perceived to work with what is known are given
more confidence.  With little describing the building block functions, coming up
with a solution to all of the minor problems in this thesis's system was based on
simplicity, good software practices and artificial intelligence research.

Through the course of the thesis work, there was a continual readjustment of the
tradeoffs. The competing design points were: complexity, implementation time,
faithfulness to the models and, when results became available, accuracy in the
results.  The initial designs were too complex and required too much time for
little gain in either of the accuracies.  After a few iterations of refining the
core idea and removing some of the parallelism complexity, a plausible model was
designed and implemented.

\section{High-level solution}

\begin{figure}[htp]     
	\begin{center}
		\includegraphics[width=0.98\linewidth]{figures/models/System}
		\caption{Proposed Design 
		\label{fig:high_level_solution}}
	\end{center}
\end{figure}

Figure \ref{fig:high_level_solution} gives a high level overview of the learning process.  
The process starts by passing in a sentence and a target word within the sentence that requires 
disambiguation. The sentence is then pre-processed in an effort to simplify data storage and 
to lower algorithm run-times.  The process involves removing stopwords, then normalizing and stemming 
the remaining words. Stopwords are words that can be removed from processing as
they add very little to the sentence. Normalizing is the removal of punctuation and case
differences.  Stemming is the act of generalizing words to their root form. 
In the sentence given in Figure \ref{fig:high_level_solution}, 
``I enjoy bass, it tastes delicious!", an example of stopwords are ``it" and ``I". The
remaining words would be transformed to lowercase, punctuation would be removed, and the sentence would be passed to the stemmer. 
The output would be: ``enjoy bass tast delici". This sentence is more compact than the original 
and still captures the essence of the sentence. The new sentence is then run through a 
``best word choice'' algorithm. The ``best word choice'' algorithm uses previously collected 
statistical information to select the words in the sentence that have the greatest chance of 
successfully completing the disambiguation. This is done by determining the frequency with which each word 
in the sentence has appeared with the target, multiplied by the number of correct disambiguations 
the word has been a part of. After the best words are selected, they are loaded 
into a memory data structure. This data structure acts as working memory, or a solution scratchpad
for the problem instance. The memory paths used to solve the problem are recorded, with 
the goal of adding useful information to the long-term memory structure, to better future queries. 
The working memory connects the current problem to existing problems. The underlying structure is 
a graph, where the nodes represent words, and the edges are weighted to promote the best path to 
search the graph for a word sense.  The working-memory graph consists of the current words, and 
adds the path each word-node used to attempt to disambiguate the target word. This information, 
along with information from the statistics engine, and the outcome are used to incorporate the 
working memory graph into the long-term memory graph.
The memory search method for disambiguating the target word is paired with a fast statistical 
algorithm and make up the two arrows in Figure \ref{fig:high_level_solution} pointing to 
``Complete?''. The fast statistical algorithm uses a simple frequency analysis of target word  
to word sense, essentially returning the most commonly seen word sense for a given word.
This method for determing word sense acts as the quick gut check. When these two   
functions have completed, they return a word sense and an associated confidence score.
These two sets of results are then passed to the ``return answer'' section which uses the
meta-component to determine which word sense is to be the final answer and what aspects of the 
collected working memory, combined with the feedback from the answer given, should be used to 
update the statistical and long-term memory component. After the 
process is complete, a check is done in the statistical engine to determine how well the 
system is optimized, seen in the ``update Meta-AI?'' box.  If the check determines that changes should 
be made, the system is modified and testing resumes.

\section{Current Model}

The high level solution mentioned above gives an executive view of the system.
This section goes into detail of what the components are, what design decisions
were made, and how the system differs from the ideal solution.

\begin{figure}[htp]     
	\begin{center}
		\includegraphics[width=0.97\linewidth]{figures/models/Component}
		\caption{Component Overview}     
	\end{center}     
	\label{fig:Component}
\end{figure}

The current model consists of three functional components and a higher level
container object.  The Statistical,
Connective, and Meta-learning components are used by the Memory component to
provide access to the various ``executive'' building blocks.

\subsection{Memory object} 

The memory object could be considered the interface to learning. It is composed of the 
``Learning Environment Data Structure''(LEDS), which is the per-problem scratch pad or working memory, 
and the interface that communicates with long-term memory and with the statistical component. 
Together they capture how the problem was solved, what was used to solve it, and in the end 
how the information in the LEDS can be used for future problem solving.

The interface component contains methods that provide the following functionality:

\begin{enumerate}     
	\item a way to load and save the state of the mind. This way, the learning
	environment can be stored for further refinement against 
	additional data sets. This more closely mimics the environment in which 
	humans learn.
	\item a training method, which given a set of data, will add the
	information to memory and attempt to optimize the structure of the data 
	for solving the problem at hand.
	\item a results method, which queries the memory components to provide an answer
	to the question given.
\end{enumerate}

Ideally this object would, dynamically
and based on prior problem solving information, design the requirements for the
solution. It would then pass the various components enough information to have
them intelligently look for the answer.  Another ideal characteristic would be
for the memory object to handle the parallel processes and synchronize
information as it was found.  This would allow biases based on unconscious
insight or new information coming into focus to change the solution path,
allowing a more elastic problem-solving capability.  These enhancements would
also bring the memory object more in line with Baddeley and Hitch's Central Executive.

\subsection{Statistical component} 

The statistical component is an attempt to
mimic the unconscious mind, a quick pattern matcher that is the first
component of memory to have a possible solution to a problem.  Tests have shown
\cite{BLINK} that the unconscious mind is quick to realize patterns, but slow to
affect conscious decisions. Ideally it would be continually collecting
statistics, running over problems, and attempting to make sense of how
disconnected information possibly fits together.

The statistical component is responsible for the following functionality:

\begin{enumerate}     
	\item a method to capture and train information related to words.  
	This method counts and indexes the words in their relation to the 
	target disambiguation. It also captures statistics on each of the 
	targeted word-disambiguation combinations.     
	\item a voting mechanism, which given a list of words and a target, uses 
	frequency and accuracy statistics of each word to vote on which disambiguation 
	of the target is most likely to be correct.
	\item a method to determine which words in a sentence are most likely to 
	lead to a successful completion.  This determines the importance of each 
	word based on how often the word has been seen in combination with the target,
	and how often the word has led to a successful disambiguation.     
	\item a fast method that, given nothing but the target word, returns the most likely disambiguation based on the most commonly seen word-sense of the target word.
\end{enumerate}

While the model is currently simplistic, this is a necessity with the constraints of current computer
hardware as it relates to the massive parallelism of the human brain and general
solution complexity.  Ideally, the statistical component or unconscious mind
would collect and index every piece of statistical information it could find 
and, using all non-busy resources, continually refine and apply new information
to existing problems.  This would provide a much more dynamic best effort guess, which would almost
certainly be a better generic problem-solving method.

\subsection{Connective component} 

The connective component models long-term
memory, a logically organized data structure that attempts to allow a deductive
problem solving process.  The connective component in combination with the
working-memory component allows for ``memory'' connections to gain strength or
to decay, increasing the success rate of determining the deductive path.

The connective component is responsible for the following functionality:

\begin{enumerate}
	\item a method to get or learn the various words.  This method activates, or loads, the long-term
	memory node of each word from the problem instance, bringing its neighbours into working memory. 
	If a word does not exist in long-term memory, a graph of its neighbours found in the problem set
	is loaded.  The word would be added through the memory object after the completion of the problem.  
	\item a voting mechanism. This method votes on which is the correct word sense by searching the memory
	graph from each word-node. The search acts in a greedy manner, returning the first possible
	disambiguation found of the target word. The word sense returned with the most results is determined 
	to be the correct word sense.

\end{enumerate}

In addition to the above functionality, the data structure itself allows for
feedback from the working memory object.  This feedback reflects the success 
of the currently completed problem, which may put a higher cost on traversing 
edges that led to an incorrect answer, or reduce the cost on edges that contributed 
to a successful answer.

The expressiveness of a graph in modelling long term memory does allow for a
natural correlation. While the form that long-term memory takes, how it is used in
problem solving, and how it works with working memory is fairly ambiguous in any
of the models, a number of improvements could be made to bring the memory more
in line with the psychological models:

\begin{enumerate}     
	\item dimensionality - Currently the long-term memory object is set up to solve just the 
	disambiguation problem; no other data outside this problem are collected 
	and no additional dimensions related to words are used to make better sense 
	of a problem. Ideally, the neighbours of a word could pivot on information 
	found in one of the additional dimensions of data collected, allowing for better 
	accuracy.
	\item unconscious access - As mentioned above, in a parallel environment 
	similar to the brain's, ideally the unconscious working away would affect 
	the memory graph (slowly), applying information learned in pattern matching 
	to the overall structure of the graph.
\end{enumerate}

\subsection{Meta-learning component} 

The meta-learning component is an attempt
to bring some of the functionality of the central executive and methodology of
problem solving to the current model. It is responsible for various meta-values
that are added into the algorithms of the other components.  This attempts to 
allow the current model to better learn how to learn. It adjusts how it  
solves problems based on trial and error. The tracked values are values that can affect 
the outcome of a problem, without changing or modifying actual problem data.

Currently tracked values include:

\begin{enumerate}     
	\item important words - the number of words to keep when evaluating a given 
	sentence for importance     
	\item graph depth - the maximum depth to which the voting algorithm should go when 
	attempting to vote using a word 
	\item graph correctness - the value by which to increment the edge weight 
	between the given node and target node when it is correct     
	\item graph incorrectness - the value by which to decrement the edge weight 
	between the given node and target node when it is incorrect     
	\item update frequency - how many solutions should pass between meta 
	algorithm updates 
\end{enumerate}

Each value contains the current numerical value, the complete history of changes
and the maximum step amount between changes.  The algorithm for meta-learning 
randomly selects a value to change, changes it, and then uses the hypothesis
testing to determine if the outcome was worth it. If it was, it changes it again; 
if it wasn't, it reverses the value and randomly selects another value to
modify.

In addition to this, the meta-learning component is also used to track how each
of the other two components performs and adjusts their voting confidences
based on past performance, ultimately giving the meta-learning component the final
say on voting success.

It also puts the working memory information learned in each problem instance
into practice, modifying the statistical and graph information contained in the
other two components.

Ideally, expanding on the meta-tracking information to include all values, and
eventually all methods and general problem solving strategies, would allow the
meta learning object to become more like the brain and move from specific
problems to general problems.  The complexity of this increases quickly, as does
the number of resources required (massive parallelism again). The necessary
information to track would grow and, more importantly, the number of training
examples required to generate good results would be very large.
Initially, genetic algorithms would be used just to get off the ground, but tackling the
executive function problems and then the specific component algorithm creation
requires something outside of the scope of this thesis.

\section{Base Models}

Now that the system has been outlined at both a high level and a component
level, an overview of the decisions that were required will help the reader to better
understand the complexity of the model. This section analyses the components
against the models, shows their shortcomings and attempts to justify a number
of the decisions made.

\subsection{Baddeley and Hitch}

In the initial implementation, the main component taken from Baddeley and
Hitch's model was the central executive.  The central executive was responsible
for scheduling tasks, coordinating the underlying systems, and making sense of
the results from the sources.  In the final implementation, with the removal of
parallelism, the tasks became more static.  Although parallelism is essential to 
modelling the brain and mind, using it to tackle only the one problem presented 
had minimal gains for the amount of effort required. The functions that made up the
original executive moved into the memory workflow.  Methods that
coordinated the system's tasks and handled the passing of values and components
between the various memory components remained, but were simplified.  With the
reduced constraints due to the serialization of tasks and workflow, the
scheduling component was removed, as dynamic execution was no
longer required.

The looping components of the model were reduced from the four
components outlined in Section 2.2.1 into one main looping structure.  This looping structure
consisted of the train, optimize and guess loop.  The training part initializes
and loads the long-term memory component and the unconscious mind's statistics
engine.  The optimize part simulates one cycle through the ``learning'' loop,
like the Baddeley and Hitch model; information is processed, connections are
made and removed, and the long-term memory structure becomes more organized.
Ideally, just as in the model, this portion of the loop would iterate several
times, strengthening the structural learning components. This process being slow
in humans, who have the luxury of gathering it over many decades, it is also quite
slow for the non-parallelized program (and, even then, work on handling
simultaneous reads and writes would be required). Finally, the guess loop uses the
metrics gathered and the long-term and statistical information created in the
optimize part to find the most likely solution to the query. Another aspect
taken from the model is that of influence between components.  Where the
Baddeley and Hitch model parallelized the input, and handled it in specific
memory objects, this project had a single point of input which was then
transferred into two models of viewing it, with the memory component and
unconscious mind component acting as the central executive.

While the overall structure is similar, it is simplified to fit.  The main
aspect of the Baddeley and Hitch model is how the multiple loops and buffers are
processed and how they interact with the various memory components.  The
phonological loop repeatedly rehearses sounds, such as language, to refresh a
phonological area of working memory.  This can be used with sounds and language
themselves, or through reading words and having the phonological loop repeat the
words, putting the words in working memory, strengthening each of the words and
the connections.  While the phonological loop and memory store
are at work, the mind can also be processing other components of the story at
the same time in the visuospatial sketchpad or the episodic buffer. The
visuospatial sketchpad allows visual information to be stored and manipulated,
and tasks involving planning, navigation and physical movement to be ordered and
manipulated.  The episodic buffer is thought to link all the forms of learning
(visual, spatial, and verbal) to a chronological timeline.  This component is also
the most important in linking to long-term memory and making semantic
connections.  Now if we take the reading example mentioned above and apply both
the visuospatial sketchpad and the episodic buffer, we gain a visual memory of
the scene in the book, which we are able to mentally move through.  We also have
an imprint of the sounds of the words and a timeline of how the scene unfolds
and a semantic understanding of the words.  The three systems working together
have a powerful effect of truly understanding the words, the characters and the
scene. With the three acting in concert together, information gained in any of
the sections is available to the mind as a whole, thus compounding the learning
ability and increasing the likelihood of making long-term and unconscious
connections.  With Baddeley and Hitch specifically stating that these components
work in parallel with one another, with any information passing or issues
handled by the central executive, the learning and understanding potential is
much more powerful than the serialized one-track solution designed.

\subsection{Cowan}

The main idea used from Cowan's memory model is the unification of long-term and
working memory through the use of focus.  The memory components have three
states; however, in each state, the memory component has the same internal
representation. The initial state, which occurs through the creation of an
instance, is done when the problem encountered requires a new memory container.
This occurs when the word encountered has not been seen before and could be
required to solve the current problem. The memory component is then used for the
problem instance, and once the instance is completed and the component is found
to be useful, the component's state is added to the long-term store. The next
state occurs while the memory component is at rest in the long-term store. When
the memory component is not required for a problem, the component lies dormant
in a long-term store. For the majority of the time, the component is in this
state.  In this state, it can be written to a file, serialized, and
compressed, mimicking the storage condition of the human brain. Finally, in the
third state, the stored memory components are required for a problem instance.
The components that gain focus are activated out of the long-term store and are
brought into the current problem instance with the information of past
experiences available. In the active state, they provide connection to other
nodes that could contain required information, and hold useful statistics used
in the problem solving algorithm.  During the problem instance, interactions
useful to solving the problem are imprinted on the active memory nodes; 
when the problem is completed, the active nodes are returned to the long-term
store, with new problem-solving statistics.

One of the limitations of the implementation, in regards to Cowan's model, is the
statistical collection and memory types.  Cowan's model allows for varying
degrees of detail as required by the degree of focus.  In the long-term store,
each component holds little information, but the connection of all of the
components can be seen.  With the first degree of focus, only approximately
seven elements are ``loaded'' into memory, but with this, the amount of
information and statistics on each of the seven elements are much greater.
While the implementation attempted to do this, the information chosen for
collection was hand-selected and focused on what was ``thought'' to be more
useful, and was stored at every level (but not necessarily used).  In order for
this to be more useful, an automated method for statistical collection would have
to be used and analyzed by the unconscious mind.  It is only then that focusing
on a few elements or a single element and allowing a comparison between the
deeper statistical information could garner a much greater level of problem
solving. Furthermore, in only capturing written text input, the deepest focus,
which loads only a single element but allows for a full view of the element, is
missing out on better connections.  Whereas humans can add the input from the
five senses to a memory component, and in doing so create a five dimensional
graph of connections, the implementation deals only in a single dimension, which
suffers from a convoluted word space and unsolvable disambiguations.

\subsection{Ericsson and Kintsch}

The important idea in Ericsson and Kintsch's work is the idea of memory linking.
The idea is that, in working memory, we are able to hold cues or links to long-term memory components.  This allows us to understand complex memories through
the use of holding important short-term concepts which can quickly bring the
more abundant long-term memories into focus.  This model works very well in
practice.  In the working model, each memory component that is loaded into short
term memory is little more than a cue to its place in the long-term memory data
structure.  The memory component contains some information about itself and a
list of its neighbours.  The algorithms for readying the working-memory
structure for problem solving first attempt to load the memory objects (and
therefore cues) that seem most relevant.  Then, in the process of solving the
problems, each component is analyzed, and if during the process of solving the
problem, any of the cues seem overly relevant, the more strongly related
neighbours are loaded into memory.  This allows the relationship of the memory
components seen during the initial look at the problem to be the starting point.
Then, using the complex collected relationships of each memory component's long-term links, it begins traversing the set of both nodes looking for similarities.
When the perceived strongest relationship links to a disambiguation that matches
the word in question, the connective component's search for an answer is
considered complete.

Another important idea in Ericsson and Kintsch's work is that there are
different retrieval structures dependent on the nature of the subject matter contained
therein.  While the working model created uses a combination of the deliberate
structure and the text comprehension structure, it is not capable of separating
the two methods or of applying the pattern and schema's structure.  Separating the
two structures currently in use would allow the model to put more importance on
links that have a higher likelihood of solving a problem.  Instead, the
deliberate structure is used on every potential item in each sentence, making
each item as important as every other item.  It does use text comprehension to
remove items that are not relevant, focusing instead on the nouns and verbs of a
sentence.  To improve the memory structure further, applying the patterns and
schema to the solutions where relevant would greatly increase connective memory
accuracy.  This would require three distinct parsers with three separate
learning engines.  The sentence parser would act as it currently does and would rate
the importance of each word.  At the same time, the deliberate memory parser
would determine if any of the words or ideas in the sentence link to any flagged
memories, triggering their addition to the solution equation.  This portion
would require a method to determine if a given memory component has some value
that an algorithm would determine to be anomalous enough to warrant a deliberate
memory link. Finally, the third parser would be the pattern and schema parser; 
this would abstract and compare the current sentence and ideas within the
sentence to other abstracted memory component collections.  This parser would
then determine likely outcome memory components or memory areas to narrow the
search.

The current model does perform a number of what the parsers described above do; 
however, a more complex memory component model would be necessary to provide the
abstractions needed to determine general problem-solving patterns.  Furthermore,
developing the algorithms to determine which memory components are more
important and should be used as a future indicator of success where possible
requires more research.

\subsection{Unconscious Mind}

The unconscious mind component was initially not a planned component.  It came
from the necessity of having an algorithm that did not require the connective
memory component to determine which words in a sentence were important. In
researching the unconscious mind, and how humans use their ``gut'' in problem
solving, it was evident that a component that tracked simple statistics was
required. The unconscious mind became the basis for determining the best words
in a sentence, the statistical memory component of the overall solution
algorithm, and assisted in the meta-learning algorithm.  While no definitive
models exist for what exactly the unconscious mind does, the Iowa gambling task
\cite{Bechara1, Bechara2} mentioned in {\it Blink} \cite{BLINK} adds credibility to an
unconscious component of the brain forming a reaction well before it enters
conscious thought. It is assumed that some form of statistical analysis is
performed and it is of great use in decision making.  The idea of the
unconscious as a statistical solution engine guided the design.  The studies on
the use of unconscious mind emphasized the difference between declarative
knowledge, which allows one to articulate reasons for choices, and procedural
knowledge, which allows repeated complex actions to be understood on an
unconscious level.  Where declarative memory takes longer to recall, the mental
processes that act on it allow more useful relative information to be made
available.  On the other hand, procedural knowledge can, over time, move from an 
unconscious understanding to a long-term reflexive understanding.  The person
is able to know something or properly mimic an action but it is done at a level
that requires little, if any, conscious thought.  Procedural knowledge is more
disconnected from neighbouring memories, and even an attempt at describing the
process is often difficult.

The implementation of the unconscious mind in the current model does attempt to
add in procedural knowledge.  However, unlike true procedural knowledge, the
implementation's version has a much more conscious process.  Currently, the model
relies on a predefined collection of statistical information.  This information is
then used in a number of predetermined algorithms; some, like the hypothesis
testing algorithm, allow for flexible and adaptable use of information that most do
not.  The ideal solution would allow for emergent behaviours in both determining
what statistics to collect and how to use the statistics to best solve the
current and future problems.  Another issue that would allow for greater success
is having the unconscious mind completely separate from the other components,
collecting the information and statistics anonymously. This would allow for both
a better collection of information and more up to date information when needed.
The unconscious mind could also spend a great deal more of its time making,
testing and redefining multiple hypotheses, without affecting the conscious work
loop.

The statistical engine in its current state tracks a number of useful properties
including: 

\begin{itemize}
	\item the number of times a word has been seen
	\item the number of times the word has been in a successfully disambiguated
	sentence
	\item the number of times the word has been used successfully in the 
	statistical and connective algorithms
	\item a list of each word that has been seen by a disambiguation, and the 
	number of times it has been seen. 
	\item a list of each disambiguation that a word has seen, and the number of 
	times it has been seen. 
\end{itemize}
